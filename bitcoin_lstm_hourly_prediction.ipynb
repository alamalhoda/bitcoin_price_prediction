{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## نصب کتابخانه‌های مورد نیاز\n",
        "در این بخش، کتابخانه‌های لازم برای اجرای مدل نصب می‌شوند."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "!pip install pandas numpy matplotlib joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## اتصال به Google Drive\n",
        "در این بخش، Google Drive متصل می‌شود تا بتوانیم داده‌ها را از آن بخوانیم و خروجی‌ها را در آن ذخیره کنیم."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## وارد کردن کتابخانه‌ها\n",
        "در این بخش، همه کتابخانه‌های مورد نیاز برای اجرای مدل وارد می‌شوند."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## تنظیم مسیر ذخیره‌سازی در Google Drive\n",
        "یک پوشه در Google Drive برای ذخیره مدل، گزارش و نمودارها ایجاد می‌کنیم."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# تنظیم مسیر ذخیره‌سازی با تاریخ و زمان\n",
        "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f'/content/drive/MyDrive/Bitcoin_Hourly_Prediction_{run_name}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "print(f\"خروجی‌ها در مسیر {save_dir} ذخیره خواهند شد.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## تعریف توابع کمکی\n",
        "در این بخش، توابع مورد نیاز برای پیش‌پردازش، آموزش و ارزیابی مدل تعریف می‌شوند."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_rsi(data, periods=14):\n",
        "    delta = data['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def calculate_rsi_for_row(last_sequence_orig, periods=14):\n",
        "    prices = last_sequence_orig[:, 0]  # ستون Close\n",
        "    delta = np.diff(prices)\n",
        "    gain = np.mean(delta[delta > 0]) if len(delta[delta > 0]) > 0 else 0\n",
        "    loss = -np.mean(delta[delta < 0]) if len(delta[delta < 0]) > 0 else 0\n",
        "    rs = gain / loss if loss != 0 else float('inf')\n",
        "    rsi = 100 - (100 / (1 + rs)) if rs != float('inf') else 50.0\n",
        "    return rsi\n",
        "\n",
        "def predict_future_days(model, last_sequence, scaler_X, scaler_y, future_hours=24):\n",
        "    print(f\"\\nپیش‌بینی قیمت‌ها برای {future_hours} ساعت آینده...\")\n",
        "    last_sequence_orig = scaler_X.inverse_transform(last_sequence.reshape(-1, last_sequence.shape[-1]))\n",
        "    future_predictions = []\n",
        "    future_dates = []\n",
        "    current_sequence = last_sequence.copy()\n",
        "    last_date = pd.to_datetime('2025-03-10 23:00:00')  # تاریخ آخرین داده\n",
        "    \n",
        "    for i in range(future_hours):\n",
        "        next_pred = model.predict(current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1]), verbose=0)\n",
        "        next_pred_orig = scaler_y.inverse_transform(next_pred)\n",
        "        future_predictions.append(next_pred_orig[0][0])\n",
        "        next_date = last_date + pd.Timedelta(hours=i+1)\n",
        "        future_dates.append(next_date)\n",
        "        \n",
        "        new_row = np.zeros((1, last_sequence.shape[-1]))\n",
        "        new_row[0, 0] = next_pred_orig[0][0]  # Close\n",
        "        new_row[0, 1] = next_pred_orig[0][0] * 1.001  # High\n",
        "        new_row[0, 2] = next_pred_orig[0][0] * 0.999  # Low\n",
        "        new_row[0, 3] = next_pred_orig[0][0]  # Open\n",
        "        new_row[0, 4] = last_sequence_orig[-1, 4]  # Volume\n",
        "        new_row[0, 5] = next_date.hour\n",
        "        new_row[0, 6] = next_date.dayofweek\n",
        "        new_row[0, 7] = next_date.month\n",
        "        new_row[0, 8] = 1 if next_date.dayofweek >= 5 else 0\n",
        "        new_row[0, 9] = (new_row[0, 0] - last_sequence_orig[-1, 0]) / last_sequence_orig[-1, 0]  # Hourly Return\n",
        "        new_row[0, 10] = (new_row[0, 0] - last_sequence_orig[-24, 0]) / last_sequence_orig[-24, 0]  # Daily Return\n",
        "        new_row[0, 11] = np.mean(np.append(last_sequence_orig[-23:, 0], new_row[0, 0]))  # SMA_24\n",
        "        new_row[0, 12] = np.mean(np.append(last_sequence_orig[-49:, 0], new_row[0, 0]))  # SMA_50\n",
        "        new_row[0, 13] = np.mean(np.append(last_sequence_orig[-199:, 0], new_row[0, 0]))  # SMA_200\n",
        "        new_row[0, 14] = calculate_rsi_for_row(np.append(last_sequence_orig[:, 0], new_row[0, 0]))  # RSI\n",
        "        new_row[0, 15] = np.std(np.append(last_sequence_orig[-11:, 0], new_row[0, 0]))  # Volatility_12h\n",
        "        new_row[0, 16] = np.std(np.append(last_sequence_orig[-23:, 0], new_row[0, 0]))  # Volatility_24h\n",
        "        new_row[0, 17] = new_row[0, 3] - last_sequence_orig[-1, 0]  # Price_Gap\n",
        "        new_row[0, 18] = new_row[0, 4] / np.mean(last_sequence_orig[-24:, 4])  # Volume_Ratio\n",
        "        new_row[0, 19] = new_row[0, 1] - new_row[0, 2]  # Price_Range\n",
        "        \n",
        "        new_row_scaled = scaler_X.transform(new_row)\n",
        "        current_sequence = np.vstack([current_sequence[1:], new_row_scaled])\n",
        "        last_sequence_orig = np.vstack([last_sequence_orig[1:], new_row])\n",
        "    \n",
        "    return np.array(future_predictions), np.array(future_dates)\n",
        "\n",
        "def load_data(file_path):\n",
        "    print(f\"\\nخواندن داده‌ها از فایل: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, skiprows=[1, 2])\n",
        "        print(f\"شکل اولیه داده‌ها: {df.shape}\")\n",
        "        print(f\"ستون‌های اولیه: {df.columns.tolist()}\\n\")\n",
        "        df = df.drop('Daily Return', axis=1)\n",
        "        df['Price'] = pd.to_datetime(df['Price'])\n",
        "        df.set_index('Price', inplace=True)\n",
        "        df.index.name = 'Date'\n",
        "        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        for col in numeric_columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        print(\"نمونه‌ای از داده‌های اولیه:\")\n",
        "        print(df.head(), \"\\n\")\n",
        "        print(\"اضافه کردن ویژگی‌های جدید...\")\n",
        "        df['Hour'] = df.index.hour\n",
        "        df['Day_of_Week'] = df.index.dayofweek\n",
        "        df['Month'] = df.index.month\n",
        "        df['Is_Weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)\n",
        "        df['Hourly_Return'] = df['Close'].pct_change()\n",
        "        df['Daily_Return'] = df['Close'].pct_change(24)\n",
        "        df['SMA_24'] = df['Close'].rolling(window=24).mean()\n",
        "        df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "        df['SMA_200'] = df['Close'].rolling(window=200).mean()\n",
        "        df['RSI'] = calculate_rsi(df, periods=14)\n",
        "        df['Volatility_12h'] = df['Hourly_Return'].rolling(window=12).std()\n",
        "        df['Volatility_24h'] = df['Hourly_Return'].rolling(window=24).std()\n",
        "        df['Price_Gap'] = df['Open'] - df['Close'].shift(1)\n",
        "        df['Volume_Ratio'] = df['Volume'] / df['Volume'].rolling(window=24).mean()\n",
        "        df['Price_Range'] = df['High'] - df['Low']\n",
        "        print(\"\\nقبل از حذف NaN:\")\n",
        "        print(f\"تعداد رکوردها: {len(df)}\")\n",
        "        print(f\"تعداد ویژگی‌ها: {len(df.columns)}\")\n",
        "        print(\"تعداد NaN در هر ستون:\")\n",
        "        print(df.isna().sum())\n",
        "        df_cleaned = df.dropna()\n",
        "        if len(df_cleaned) == 0:\n",
        "            raise ValueError(\"پس از حذف NaN هیچ داده‌ای باقی نمانده است!\")\n",
        "        print(\"\\nپس از حذف NaN:\")\n",
        "        print(f\"تعداد رکوردها: {len(df_cleaned)}\")\n",
        "        print(f\"تعداد ویژگی‌ها: {len(df_cleaned.columns)}\")\n",
        "        print(f\"بازه زمانی: از {df_cleaned.index[0]} تا {df_cleaned.index[-1]}\\n\")\n",
        "        return df_cleaned\n",
        "    except Exception as e:\n",
        "        print(f\"خطا در خواندن یا پردازش داده‌ها: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def save_plots_and_report(save_dir, history, train_metrics, test_metrics, future_predictions, future_dates, data, train_predictions, test_predictions, y_train_original, y_test_original):\n",
        "    # ذخیره نمودار Loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'loss_plot.png'))\n",
        "    plt.close()\n",
        "    \n",
        "    # ذخیره نمودار پیش‌بینی\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(data.index[-len(y_test_original):], y_test_original, label='Actual Price')\n",
        "    plt.plot(data.index[-len(test_predictions):], test_predictions, label='Predicted Price')\n",
        "    plt.plot(future_dates, future_predictions, '--', label='Future Predictions (24 hours)')\n",
        "    plt.title('Bitcoin Hourly Price Prediction')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price (USD)')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'price_prediction_plot.png'))\n",
        "    plt.close()\n",
        "    \n",
        "    # ذخیره گزارش\n",
        "    report_path = os.path.join(save_dir, 'evaluation_report.txt')\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"گزارش ارزیابی مدل پیش‌بینی قیمت بیت‌کوین ساعتی\\n\")\n",
        "        f.write(\"==================================================\\n\\n\")\n",
        "        f.write(\"معیارهای آموزشی:\\n\")\n",
        "        f.write(f\"MSE: {train_metrics['MSE']:.2f}\\n\")\n",
        "        f.write(f\"RMSE: {train_metrics['RMSE']:.2f}\\n\")\n",
        "        f.write(f\"MAE: {train_metrics['MAE']:.2f}\\n\")\n",
        "        f.write(f\"R²: {train_metrics['R2']:.4f}\\n\\n\")\n",
        "        f.write(\"معیارهای تست:\\n\")\n",
        "        f.write(f\"MSE: {test_metrics['MSE']:.2f}\\n\")\n",
        "        f.write(f\"RMSE: {test_metrics['RMSE']:.2f}\\n\")\n",
        "        f.write(f\"MAE: {test_metrics['MAE']:.2f}\\n\")\n",
        "        f.write(f\"R²: {test_metrics['R2']:.4f}\\n\\n\")\n",
        "        f.write(\"پیش‌بینی قیمت برای ۲۴ ساعت آینده:\\n\")\n",
        "        for date, price in zip(future_dates, future_predictions):\n",
        "            f.write(f\"{date}: ${price:.2f}\\n\")\n",
        "\n",
        "def predict_with_lstm(data, lag_days=24, epochs=50, batch_size=32, validation_split=0.1,\n",
        "                     dropout=0.3, early_stopping_patience=10, restore_best_weights=True,\n",
        "                     optimizer='adam', loss='mse'):\n",
        "    start_time = time.time()\n",
        "    model_params = {\n",
        "        'lag_days': lag_days,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'validation_split': validation_split,\n",
        "        'dropout': dropout,\n",
        "        'early_stopping_patience': early_stopping_patience,\n",
        "        'restore_best_weights': restore_best_weights,\n",
        "        'optimizer': optimizer,\n",
        "        'loss': loss\n",
        "    }\n",
        "    print(\"\\nبررسی داده‌های ورودی:\")\n",
        "    print(f\"شکل داده‌ها: {data.shape}\")\n",
        "    print(f\"ستون‌های موجود: {data.columns.tolist()}\")\n",
        "    X = data.drop(['Close'], axis=1)\n",
        "    y = data['Close']\n",
        "    train_size = int(len(X) * 0.88)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "    X_lstm_train = []\n",
        "    y_lstm_train = []\n",
        "    for i in range(lag_days, len(X_train_scaled)):\n",
        "        X_lstm_train.append(X_train_scaled[i-lag_days:i])\n",
        "        y_lstm_train.append(y_train_scaled[i])\n",
        "    X_lstm_test = []\n",
        "    y_lstm_test = []\n",
        "    for i in range(lag_days, len(X_test_scaled)):\n",
        "        X_lstm_test.append(X_test_scaled[i-lag_days:i])\n",
        "        y_lstm_test.append(y_test_scaled[i])\n",
        "    X_lstm_train, y_lstm_train = np.array(X_lstm_train), np.array(y_lstm_train)\n",
        "    X_lstm_test, y_lstm_test = np.array(X_lstm_test), np.array(y_lstm_test)\n",
        "    print(f\"Length of X_lstm_train: {len(X_lstm_train)}, Length of y_lstm_train: {len(y_lstm_train)}\")\n",
        "    print(f\"Length of X_lstm_test: {len(X_lstm_test)}, Length of y_lstm_test: {len(y_lstm_test)}\")\n",
        "    \n",
        "    model = Sequential([\n",
        "        Input(shape=(lag_days, X.shape[1])),\n",
        "        LSTM(128, activation='tanh', return_sequences=True,\n",
        "             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        LSTM(64, activation='tanh', return_sequences=True,\n",
        "             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        LSTM(32, activation='tanh',\n",
        "             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu',\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=early_stopping_patience,\n",
        "        restore_best_weights=restore_best_weights\n",
        "    )\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "    history = model.fit(\n",
        "        X_lstm_train, y_lstm_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"\\nزمان کل پردازش: {processing_time:.2f} ثانیه\")\n",
        "    train_predictions = model.predict(X_lstm_train, verbose=0)\n",
        "    test_predictions = model.predict(X_lstm_test, verbose=0)\n",
        "    train_predictions = scaler_y.inverse_transform(train_predictions)\n",
        "    test_predictions = scaler_y.inverse_transform(test_predictions)\n",
        "    y_train_original = scaler_y.inverse_transform(y_lstm_train)\n",
        "    y_test_original = scaler_y.inverse_transform(y_lstm_test)\n",
        "    train_metrics = {\n",
        "        'MSE': mean_squared_error(y_train_original, train_predictions),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_train_original, train_predictions)),\n",
        "        'MAE': mean_absolute_error(y_train_original, train_predictions),\n",
        "        'R2': r2_score(y_train_original, train_predictions)\n",
        "    }\n",
        "    test_metrics = {\n",
        "        'MSE': mean_squared_error(y_test_original, test_predictions),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test_original, test_predictions)),\n",
        "        'MAE': mean_absolute_error(y_test_original, test_predictions),\n",
        "        'R2': r2_score(y_test_original, test_predictions)\n",
        "    }\n",
        "    print(\"\\nنتایج ارزیابی مدل:\")\n",
        "    print(\"معیارهای آموزش:\")\n",
        "    for metric, value in train_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    print(\"\\nمعیارهای تست:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    last_sequence = X_lstm_test[-1:]\n",
        "    future_predictions, future_dates = predict_future_days(model, last_sequence, scaler_X, scaler_y)\n",
        "    print(\"\\nپیش‌بینی قیمت برای ۲۴ ساعت آینده:\")\n",
        "    for date, price in zip(future_dates, future_predictions):\n",
        "        print(f\"{date.strftime('%Y-%m-%d %H:%M:%S')}: ${price:.2f}\")\n",
        "    model_path = os.path.join(save_dir, 'lstm_model.h5')\n",
        "    model.save(model_path)\n",
        "    scaler_X_path = os.path.join(save_dir, 'scaler_X.pkl')\n",
        "    scaler_y_path = os.path.join(save_dir, 'scaler_y.pkl')\n",
        "    joblib.dump(scaler_X, scaler_X_path)\n",
        "    joblib.dump(scaler_y, scaler_y_path)\n",
        "    save_plots_and_report(save_dir, history, train_metrics, test_metrics, future_predictions, future_dates, data, train_predictions, test_predictions, y_train_original, y_test_original)\n",
        "    return model, scaler_X, scaler_y, model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## بارگذاری داده‌ها و اجرای مدل\n",
        "داده‌ها از Google Drive بارگذاری می‌شوند و مدل آموزش داده می‌شود."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/bitcoin_20170101_20250310_1h.csv'\n",
        "data = load_data(file_path)\n",
        "model, scaler_X, scaler_y, model_params = predict_with_lstm(\n",
        "    data,\n",
        "    lag_days=24,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    dropout=0.3,\n",
        "    early_stopping_patience=10,\n",
        "    restore_best_weights=True,\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## نکته\n",
        "برای سرعت بیشتر، مطمئن شوید که از GPU استفاده می‌کنید:\n",
        "- به منوی Runtime > Change runtime type بروید.\n",
        "- در بخش Hardware accelerator، گزینه GPU را انتخاب کنید.\n",
        "\n",
        "فایل‌های خروجی (مدل، اسکالرها، گزارش و نمودارها) در مسیر زیر در Google Drive ذخیره شده‌اند:\n",
        "`/content/drive/MyDrive/Bitcoin_Hourly_Prediction_{run_name}`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}